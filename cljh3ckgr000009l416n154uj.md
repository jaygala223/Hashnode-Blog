---
title: "Introduction to Few Shot Learning (FSL)"
seoTitle: "A brief introduction to few shot learning"
datePublished: Thu Jun 29 2023 11:57:40 GMT+0000 (Coordinated Universal Time)
cuid: cljh3ckgr000009l416n154uj
slug: introduction-to-few-shot-learning-fsl
tags: artificial-intelligence, machine-learning, deep-learning, meta-learning, few-shot-learning

---

Popular deep-learning models like **Resnet18, Resnet50, VGG16**, etc. have achieved outstanding results in various image recognition tasks on benchmark datasets. These neural networks have been trained on mammoth datasets containing millions of labeled examples. However, in many real-life scenarios, acquiring such vast amounts of labeled data is impossible.

Consider a scenario where we want to train a deep learning model to classify diseases based on medical images. Data availability is often an issue in the medical field, and gathering large labeled datasets can be daunting and expensive. Moreover, labeling medical images requires expertise from healthcare professionals, making the labeling process even more challenging and time-consuming.

The question arises: How can we effectively train deep learning models for disease classification and other related tasks with only a few labeled examples? This is where **few-shot learning** comes in.

**Few-shot learning**, as the name suggests, addresses the problem of learning from only a few labeled examples. When we see one image of an orange and one of an apple, we can distinguish apples from oranges almost always. This is precisely what FSL tries to mimic. FSL aims to bring us closer to the human brain in terms of efficiency.

Before moving on let's take a look at the lingo in FSL

### FSL Lingo

1. **N ways** - The number of unique classes that the model will see during training
    
2. **K shots** - The number of examples per class. K is usually kept &lt;10 for it to be considered FSL
    
3. **Support set** - The support set is a collection of *(N\*K)* samples selected randomly from the training set. This support set is used in training.
    
4. **Query set** - Often denoted by **Q** (indicating the number of query images per selected class)**,** the query set is a collection of *(Q\*N)* samples selected randomly from the training set (but these examples do not overlap with the support set examples). The Query set is used to evaluate the performance of the model on the support set.
    

internet image here ðŸ‘‡ (credit to authors [img\_link](https://www.kdnuggets.com/2020/03/few-shot-image-classification-meta-learning.html))

![Few-Shot Image Classification with Meta-Learning - KDnuggets](https://images.ctfassets.net/be04ylp8y0qc/bZhboqYXfYeW4I88xmMNv/7c5efdc368206feaad045c674b1ced95/1_AteD0yXLkQ1BbjQTB3Ytwg.png?fm=jpg align="left")

1. **Episode** - (a.k.a task) It is a single training / testing instance in an FSL setup. Each task has a support and query set. In every task, the model is expected to learn representations from the support set and then perform well on the query images. Think of it as the "number of batches" per epoch.
    

Now let's check out the different approaches there are to FSL

There are also a few variations that we will be exploring later in the series:

1. **Zero-Shot Learning:** K = 0. The model learns to classify examples with zero-labeled examples
    
2. **One-Shot Learning:** K = 1. The model is trained with only one labeled example per class.
    

It is as cool as it sounds!

### Approaches

There are 2 main approaches in FSL:

1. **Metric-based FS learning:**
    
    1. Here, we try to learn metrics such as distance or similarity between similar and dissimilar examples and the models try to learn which is which.
        
    2. The main process is to feed some samples to the model --&gt; The model computes some features --&gt; Model calculates distance / similarity / some-other-metric --&gt; Tell it whether they are similar or not --&gt; model learns --&gt; tries to do well on query samples
        
    3. Special loss functions such as Triplet Loss, Constrastive Loss, etc. are also used to encourage similarity / dissimilarity learning.
        
    4. Popular models include Prototypical nets, Relation nets, Siamese nets, etc.
        
2. **Meta Learning:**
    
    1. Meta learning is cooler. Meta learning is basically "learning to learn."
        
    2. It involves a meta learner that captures the commonalities across similar tasks and a task-specific learner which is fine-tuning parameters on new tasks
        
    3. Meta learning, in theory, enables the models to generalize to tasks that it was never designed to work for thus making it super versatile and adaptable.
        
    4. The most popular algorithm is **MAML** by [Finn et al](https://arxiv.org/abs/1703.03400).
        

### Conclusion and Future Scope

While Few Shot Learning has shown great promise, it is far from perfect. Thus, it presents many challenges as well as opportunities for research. In **computer vision**, FSL is being used for **image classification** primarily but there have been efforts to expand it to **object detection** as well as **image segmentation**.

FSL also finds its use in **natural language applications** such as **text classification** and **sentiment analysis**. It will be exciting to see how FSL evolves in the coming years!

The next blog will probably be on **Siamese Networks for one-shot learning**. Stay tuned!